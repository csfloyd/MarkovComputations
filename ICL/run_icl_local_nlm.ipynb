{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MARKOV ICL - CLASSIFICATION (Softmax Output)\n",
      "======================================================================\n",
      "K=256, D=2, N=3, B=1, nodes=6\n",
      "Method: newton, Temperature: 1.0\n",
      "======================================================================\n",
      "Device: cpu\n",
      "\n",
      "Creating GMM with discrete labels...\n",
      "  GMM: 256 classes with labels randomly assigned from {1, ..., 256}\n",
      "  First 10 class labels: [249. 186. 119. 246. 114. 171. 109.   9. 190. 144.]\n",
      "\n",
      "Generating data...\n",
      "\n",
      "Creating model...\n",
      "  Initialized Nonlinear Markov ICL model (L=256 classes, attention over 3 context items)\n",
      "  Nonlinear dynamics: W p + p Y p = 0\n",
      "  Label modulation: False\n",
      "  Base rates learnable: False\n",
      "  Base mask value: -inf\n",
      "  Sparsity K: rho_edge=0.200, rho_all=0.200\n",
      "  Sparsity L: rho_edge=0.000, rho_all=0.000\n",
      "  Sparsity base_W: rho_edge=1.000\n",
      "  Sparsity base_Y: rho_edge=0.000\n",
      "  K_params sparsity: 0.976 (7/288 active)\n",
      "  L_params sparsity: 1.000 (0/1728 active)\n",
      "  base_W sparsity: 0.000 (36/36 active)\n",
      "  base_Y sparsity: 1.000 (0/216 active)\n",
      "  Parameters: 2,034\n",
      "\n",
      "Training...\n",
      "======================================================================\n",
      "Epoch  10 | Train: 38.07% | Val: 42.05% | IWL: 1.60% | ICL: 37.60%\n",
      "Epoch  20 | Train: 41.72% | Val: 42.10% | IWL: 0.60% | ICL: 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Import from refactored modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from new modular structure\n",
    "from data_generation import GaussianMixtureModel, generate_icl_gmm_data, generate_iwl_gmm_data\n",
    "from datasets import ICLGMMDataset, collate_fn\n",
    "from models import MatrixTreeMarkovICL, RandomPolynomialICL, NonlinearMarkovICL\n",
    "from training import train_model\n",
    "from evaluation import test_icl\n",
    "from config import ExperimentConfig\n",
    "\n",
    "# ============================================================\n",
    "# Data Generation Parameters\n",
    "# ============================================================\n",
    "L = 256                      # Number of output classes\n",
    "K = 256                      # Number of GMM classes for data generation\n",
    "D = 2                        # Dimension of input features\n",
    "N = 3                        # Number of context examples per task\n",
    "B = 1                        # Burstiness parameter (zipfian sampling weight)\n",
    "epsilon = 1e-3               # Within-class noise (standard deviation)\n",
    "seed = 20                    # Random seed for reproducibility\n",
    "exact_copy = True            # If True, query is exact copy of a context item\n",
    "shuffle_context = True       # Whether to shuffle context order during training\n",
    "offset = 0.0                 # Offset applied to GMM centers\n",
    "min_max_choice = None        # Optional constraint on min/max class indices\n",
    "unique_labels = False        # If True, ensure all context labels are unique\n",
    "\n",
    "# ============================================================\n",
    "# Model Architecture Parameters\n",
    "# ============================================================\n",
    "n_nodes = 6                  # Number of nodes in the Markov chain\n",
    "transform_func = 'exp'       # Transformation function: 'exp', 'relu', or 'elu'\n",
    "learn_base_rates = False     # If True, allow gradient updates to unmasked base rates\n",
    "\n",
    "# ============================================================\n",
    "# Sparsity Parameters - K_params (context-dependent modulation)\n",
    "# ============================================================\n",
    "sparsity_rho_edge_K = 0.2    # Fraction of (i,j) edges with K parameters (0.0 = all masked)\n",
    "sparsity_rho_all_K = 0.2     # Fraction of individual K parameters to keep (0.0 = all masked)\n",
    "\n",
    "# ============================================================\n",
    "# Sparsity Parameters - L_params (nonlinear interactions)\n",
    "# ============================================================\n",
    "sparsity_rho_edge_L = 0.0    # Fraction of (i,j,k) triplets with L parameters\n",
    "sparsity_rho_all_L = 0.0    # Fraction of individual L parameters to keep\n",
    "\n",
    "# ============================================================\n",
    "# Sparsity Parameters - Base Rates\n",
    "# ============================================================\n",
    "sparsity_rho_edge_base_W = 1.0   # Fraction of (i,j) edges with base rates in W\n",
    "sparsity_rho_edge_base_Y = 0.0   # Fraction of (i,j,k) triplets with base rates in Y\n",
    "base_mask_value = float('-inf')            # Value for masked base rates: 0.0 (no bias) or float('-inf') (disable edge)\n",
    "\n",
    "# ============================================================\n",
    "# Training Parameters\n",
    "# ============================================================\n",
    "epochs = 30                  # Number of training epochs\n",
    "lr = 0.0025                  # Learning rate\n",
    "batch_size = 64              # Batch size for training\n",
    "train_samples = 25000        # Number of training samples\n",
    "val_samples = 2000           # Number of validation samples\n",
    "\n",
    "# ============================================================\n",
    "# Inference Parameters\n",
    "# ============================================================\n",
    "method = 'newton'            # Steady-state solver: 'newton', 'direct_solve', 'matrix_tree', 'linear_solver'\n",
    "temperature = 1.0            # Softmax temperature for attention\n",
    "\n",
    "# ============================================================\n",
    "# Combined Parameter Dictionary\n",
    "# ============================================================\n",
    "params = {\n",
    "    # Data generation\n",
    "    'K': K,\n",
    "    'L': L,\n",
    "    'D': D,\n",
    "    'N': N,\n",
    "    'B': B,\n",
    "    'epsilon': epsilon,\n",
    "    'seed': seed,\n",
    "    'exact_copy': exact_copy,\n",
    "    'shuffle_context': shuffle_context,\n",
    "    'offset': offset,\n",
    "    'min_max_choice': min_max_choice,\n",
    "    'unique_labels': unique_labels,\n",
    "    \n",
    "    # Model architecture\n",
    "    'n_nodes': n_nodes,\n",
    "    'transform_func': transform_func,\n",
    "    'learn_base_rates': learn_base_rates,\n",
    "            \n",
    "    # Sparsity - K_params\n",
    "    'sparsity_rho_edge_K': sparsity_rho_edge_K,\n",
    "    'sparsity_rho_all_K': sparsity_rho_all_K,\n",
    "    \n",
    "    # Sparsity - L_params\n",
    "    'sparsity_rho_edge_L': sparsity_rho_edge_L,\n",
    "    'sparsity_rho_all_L': sparsity_rho_all_L,\n",
    "    \n",
    "    # Sparsity - Base rates\n",
    "    'sparsity_rho_edge_base_W': sparsity_rho_edge_base_W,\n",
    "    'sparsity_rho_edge_base_Y': sparsity_rho_edge_base_Y,\n",
    "    'base_mask_value': base_mask_value,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'batch_size': batch_size,\n",
    "    'train_samples': train_samples,\n",
    "    'val_samples': val_samples,\n",
    "    \n",
    "    # Inference\n",
    "    'method': method,\n",
    "    'temperature': temperature\n",
    "}\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MARKOV ICL - CLASSIFICATION (Softmax Output)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"K={params['K']}, D={params['D']}, N={params['N']}, B={params['B']}, nodes={params['n_nodes']}\")\n",
    "print(f\"Method: {params['method']}, Temperature: {params['temperature']}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(params['seed'])\n",
    "np.random.seed(params['seed'])\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Create GMM with discrete labels (1 to L)\n",
    "print(\"Creating GMM with discrete labels...\")\n",
    "gmm = GaussianMixtureModel(K=params['K'], D=params['D'], L=params['L'], epsilon=params['epsilon'], seed=params['seed'], offset=params['offset'])\n",
    "print(f\"  GMM: {params['K']} classes with labels randomly assigned from {{1, ..., {params['L']}}}\")\n",
    "print(f\"  First 10 class labels: {gmm.class_to_label[:min(10, params['K'])].numpy()}\")\n",
    "\n",
    "# Generate data\n",
    "print(\"\\nGenerating data...\")\n",
    "train_data = generate_icl_gmm_data(gmm, params['train_samples'], params['N'], \n",
    "                                   novel_classes=False, exact_copy=params['exact_copy'], \n",
    "                                   B=params['B'], L=params['L'], shuffle_context=params['shuffle_context'], min_max_choice=params['min_max_choice'], unique_labels = params['unique_labels'])\n",
    "val_data = generate_icl_gmm_data(gmm, params['val_samples'], params['N'], \n",
    "                                 novel_classes=False, exact_copy=params['exact_copy'], \n",
    "                                 B=params['B'], L=params['L'], shuffle_context=params['shuffle_context'], min_max_choice=params['min_max_choice'], unique_labels = params['unique_labels'])\n",
    "\n",
    "train_loader = DataLoader(ICLGMMDataset(train_data), batch_size=params['batch_size'],\n",
    "                          shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ICLGMMDataset(val_data), batch_size=params['batch_size'],\n",
    "                       collate_fn=collate_fn)\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = NonlinearMarkovICL(n_nodes=params['n_nodes'], z_dim=params['D'], \n",
    "                           L=params['L'], N=params['N'], \n",
    "                           learn_base_rates=params['learn_base_rates'], \n",
    "                           transform_func=params['transform_func'],\n",
    "                           sparsity_rho_edge_K=params['sparsity_rho_edge_K'], \n",
    "                           sparsity_rho_all_K=params['sparsity_rho_all_K'],\n",
    "                           sparsity_rho_edge_L=params['sparsity_rho_edge_L'], \n",
    "                           sparsity_rho_all_L=params['sparsity_rho_all_L'],\n",
    "                           sparsity_rho_edge_base_W=params['sparsity_rho_edge_base_W'],\n",
    "                           sparsity_rho_edge_base_Y=params['sparsity_rho_edge_base_Y'],\n",
    "                           base_mask_value=params['base_mask_value'])\n",
    "\n",
    "# Train with ICL/IWL tracking\n",
    "start_time = time.time()\n",
    "print(\"\\nTraining...\")\n",
    "print(\"=\"*70)\n",
    "history = train_model(model, train_loader, val_loader, device, \n",
    "                     n_epochs=params['epochs'], lr=params['lr'], \n",
    "                     method=params['method'], temperature=params['temperature'],\n",
    "                     gmm=gmm, N=params['N'], B=params['B'], \n",
    "                     L=params['L'], exact_copy=params['exact_copy'],\n",
    "                     eval_frequency=1, n_eval_samples=500, min_max_choice=params['min_max_choice'], unique_labels = params['unique_labels'])\n",
    "                     \n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Test\n",
    "results = test_icl(model, gmm, params['N'], device, n_samples=1000, \n",
    "                  exact_copy=params['exact_copy'], B=params['B'], \n",
    "                  method=params['method'], L=params['L'],\n",
    "                  temperature=params['temperature'], shuffle_context=params['shuffle_context'], min_max_choice=params['min_max_choice'], unique_labels = params['unique_labels'])\n",
    "\n",
    "# # Save model\n",
    "# os.makedirs('results', exist_ok=True)\n",
    "# model_path = f'results/markov_icl_gmm_K{config.K}_N{config.N}_classification_T{config.temperature:.1f}.pt'\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "# print(f\"\\n✓ Saved: {model_path}\")\n",
    "\n",
    "# Plot training history with ICL/IWL metrics\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Extract ICL/IWL metrics (filter out None values)\n",
    "    epochs_eval = [i+1 for i, val in enumerate(history['iwl_acc']) if val is not None]\n",
    "    iwl_acc = [val for val in history['iwl_acc'] if val is not None]\n",
    "    icl_acc = [val for val in history['icl_acc'] if val is not None]\n",
    "    #icl_swap_acc = [val for val in history['icl_swap_acc'] if val is not None]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Plot 1: Training/Val Accuracy\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(history['train_acc'], label='Train Accuracy', alpha=0.7)\n",
    "    ax1.plot(history['val_acc'], label='Val Accuracy', alpha=0.7)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('Training Progress')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # Plot 2: ICL vs IWL over time\n",
    "    ax2 = axes[0, 1]\n",
    "    if len(iwl_acc) > 0:\n",
    "        ax2.plot(epochs_eval, iwl_acc, 'o-', label='IWL (In-Weight)', linewidth=2, markersize=6)\n",
    "        ax2.plot(epochs_eval, icl_acc, 's-', label='ICL (In-Context)', linewidth=2, markersize=6)\n",
    "        #ax2.plot(epochs_eval, icl_swap_acc, '^-', label='ICL Label Swap', linewidth=2, markersize=6)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('ICL vs IWL During Training')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim([0, 105])\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No ICL/IWL data\\n(set gmm parameter)', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "    \n",
    "    # Plot 3: Final test results\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.bar(['In-Distribution', 'Novel Classes'], \n",
    "            [results['in_dist'], results['novel_classes']],\n",
    "            color=['skyblue', 'coral'])\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('Final Test Results')\n",
    "    ax3.axhline(y=80, color='g', linestyle='--', label='Good threshold (80%)', alpha=0.5)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.set_ylim([0, 100])\n",
    "    \n",
    "    # Plot 4: Training/Val Loss\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
    "    ax4.plot(history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.set_title('Training Loss')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    if len(iwl_acc) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ICL/IWL SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Final IWL Accuracy:        {iwl_acc[-1]:.2f}%\")\n",
    "        print(f\"Final ICL Accuracy:  {icl_acc[-1]:.2f}%\")\n",
    "        #print(f\"Final ICL Swap Accuracy:   {icl_swap_acc[-1]:.2f}%\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n(Install matplotlib to see training plots)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vz/mzz963fn35g6stbyn1w_x0jc0000gn/T/ipykernel_49792/1506016941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a single example from the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add batch dimension and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (1, N+1, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a single example from the training dataset\n",
    "z_seq, labels, targets = train_loader.dataset[2000]\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "z_seq = z_seq.unsqueeze(0).to(device)  # Shape: (1, N+1, D)\n",
    "labels = labels.unsqueeze(0).to(device)  # Shape: (1, N)\n",
    "targets = targets.unsqueeze(0).to(device).long() - 1  # Shape: (1,)\n",
    "\n",
    "print(\"Example input:\")\n",
    "print(f\"z_seq shape: {z_seq.shape}\")\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "print(f\"z_seq:\\n{z_seq}\")\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "# Flatten z_seq as the model expects\n",
    "z_flat = z_seq.reshape(1, -1)  # Shape: (1, (N+1)*D)\n",
    "print(f\"\\nFlattened z shape: {z_flat.shape}\")\n",
    "\n",
    "# Compute the rate matrix W and rate tensor Y\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    W_mat = model.compute_rate_matrix_W(z_flat)\n",
    "    Y_tensor = model.compute_rate_tensor_Y(z_flat)\n",
    "\n",
    "print(f\"\\nRate matrix W shape: {W_mat.shape}\")\n",
    "print(f\"Rate matrix W:\\n{W_mat[0]}\")  # Print the first (and only) matrix in the batch\n",
    "\n",
    "# Verify that columns sum to approximately zero (as they should for a rate matrix)\n",
    "col_sums = W_mat[0].sum(dim=0)\n",
    "print(f\"\\nColumn sums of W (should be ~0): {col_sums}\")\n",
    "\n",
    "print(f\"\\nRate tensor Y shape: {Y_tensor.shape}\")\n",
    "print(f\"Rate tensor Y[0] (first batch):\\n{Y_tensor[0]}\")\n",
    "\n",
    "# Verify the constraint: Y[j,j,k] = -sum_{i≠j} Y[i,j,k]\n",
    "# Check for j=0, k=0 as an example\n",
    "j, k = 0, 0\n",
    "sum_off_diag = Y_tensor[0, :, j, k].sum() - Y_tensor[0, j, j, k]  # Sum excluding diagonal\n",
    "print(f\"\\nConstraint check for Y[j={j},j={j},k={k}]:\")\n",
    "print(f\"  Y[{j},{j},{k}] = {Y_tensor[0, j, j, k].item():.6f}\")\n",
    "print(f\"  -Sum_{{i≠{j}}} Y[i,{j},{k}] = {-sum_off_diag.item():.6f}\")\n",
    "print(f\"  Difference (should be ~0): {(Y_tensor[0, j, j, k] + sum_off_diag).item():.6e}\")\n",
    "\n",
    "# Compute steady state using nonlinear solver\n",
    "n_iter = 50\n",
    "step_size = 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    p_steady = model.direct_solve_steady_state(W_mat, Y_tensor, n_iter=n_iter, step_size=step_size)\n",
    "\n",
    "print(f\"\\nSteady state distribution (π):\")\n",
    "print(f\"Shape: {p_steady.shape}\")\n",
    "print(f\"π = {p_steady[0]}\")\n",
    "print(f\"Sum of π (should be 1.0): {p_steady[0].sum().item()}\")\n",
    "\n",
    "# Verify the steady state: W p + p Y p should be approximately 0\n",
    "with torch.no_grad():\n",
    "    # Linear term: W @ p\n",
    "    linear_term = torch.einsum(\"bij,bj->bi\", W_mat, p_steady)\n",
    "    \n",
    "    # Quadratic term: sum_{j,k} Y[i,j,k] * p_j * p_k\n",
    "    outer = p_steady.unsqueeze(2) * p_steady.unsqueeze(1)  # (batch_size, n, n)\n",
    "    quadratic_term = torch.einsum(\"bijk,bjk->bi\", Y_tensor, outer)\n",
    "    \n",
    "    # Total should be ~0\n",
    "    total_drift = linear_term + quadratic_term\n",
    "    \n",
    "print(f\"\\nSteady state verification (W p + p Y p):\")\n",
    "print(f\"Linear term (W @ p): {linear_term[0]}\")\n",
    "print(f\"Quadratic term (p Y p): {quadratic_term[0]}\")\n",
    "print(f\"Total (should be ~0): {total_drift[0]}\")\n",
    "print(f\"Max absolute drift: {torch.abs(total_drift[0]).max().item():.6e}\")\n",
    "\n",
    "# Compute context position scores: q_m = Σ_k B_{k,m} * π_k\n",
    "with torch.no_grad():\n",
    "    q = torch.matmul(p_steady, model.B)  # (batch_size, N)\n",
    "    attention = torch.softmax(q / temperature, dim=1)\n",
    "\n",
    "print(f\"\\nContext position scores (q):\")\n",
    "print(f\"q = {q[0]}\")\n",
    "print(f\"\\nAttention weights over context positions:\")\n",
    "print(f\"attention = {attention[0]}\")\n",
    "print(f\"Sum of attention (should be 1.0): {attention[0].sum().item()}\")\n",
    "\n",
    "# Optional: Also compute the full forward pass to see final output\n",
    "with torch.no_grad():\n",
    "    logits = model(z_seq, labels, method='direct_solve', temperature=temperature, \n",
    "                   n_iter=n_iter, step_size=step_size)\n",
    "    probs = torch.exp(logits)  # Convert log-probs to probs\n",
    "    predicted_class = logits.argmax(dim=1) + 1  # Convert back to 1-indexed\n",
    "    \n",
    "print(f\"\\nModel output:\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Top 5 logits: {logits[0].topk(5)}\")\n",
    "print(f\"Top 5 probabilities: {probs[0].topk(5)}\")\n",
    "print(f\"Predicted class: {predicted_class.item()}\")\n",
    "print(f\"True class: {targets.item() + 1}\")\n",
    "print(f\"Correct: {predicted_class.item() == targets.item() + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rate matrix W shape: torch.Size([1, 3, 3])\n",
      "Rate matrix W:\n",
      "tensor([[-2.7088e+02,  1.5343e+00,  4.2405e-01],\n",
      "        [ 4.6324e-01, -1.5737e+00,  8.0553e-01],\n",
      "        [ 2.7042e+02,  3.9341e-02, -1.2296e+00]])\n",
      "Time taken: 0.17818021774291992 seconds\n",
      "\n",
      "Steady state distribution (π):\n",
      "Shape: torch.Size([1, 3])\n",
      "π = tensor([0.0112, 0.0499, 0.9389])\n",
      "Sum of π (should be 1.0): 1.0\n",
      "\n",
      "Steady state verification (W p + p Y p):\n",
      "Linear term (W @ p): tensor([-2.5631,  0.6831,  1.8801])\n",
      "Quadratic term (p Y p): tensor([ 2.5631, -0.6831, -1.8801])\n",
      "Total (should be ~0): tensor([0., 0., 0.])\n",
      "Max absolute drift: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def direct_solve_steady_state_ext(model, W_batch, Y_batch, n_iter=50, step_size=0.05, eps=1e-12):\n",
    "        \"\"\"\n",
    "        Solve nonlinear steady state using fixed-point iteration (gradient descent).\n",
    "        \n",
    "        Solves: W p + p Y p = 0 with sum(p) = 1 and p >= 0\n",
    "        where (p Y p)_i = sum_{j,k} Y[i,j,k] p_j p_k\n",
    "        \n",
    "        Uses gradient descent to minimize ||W p + p Y p||, projecting onto the probability simplex.\n",
    "        \n",
    "        Args:\n",
    "            W_batch: (batch_size, n_nodes, n_nodes) - linear rate matrix\n",
    "            Y_batch: (batch_size, n_nodes, n_nodes, n_nodes) - nonlinear rate tensor\n",
    "            n_iter: Number of fixed-point iterations (default: 50)\n",
    "            step_size: Step size for gradient descent (default: 0.1)\n",
    "            eps: Minimum value for clamping (default: 1e-12)\n",
    "            \n",
    "        Returns:\n",
    "            p_batch: (batch_size, n_nodes) - steady state distributions\n",
    "        \"\"\"\n",
    "        batch_size, n = W_batch.shape[0], model.n_nodes\n",
    "        device = W_batch.device\n",
    "        \n",
    "        # Initialize from uniform distribution\n",
    "        p_batch = torch.full((batch_size, n), 1.0/n, device=device)\n",
    "        \n",
    "        for _ in range(n_iter):\n",
    "            # Compute quadratic term: Q_i = sum_{j,k} Y[i,j,k] p_j p_k\n",
    "            # Outer product: p_j * p_k\n",
    "            outer = p_batch.unsqueeze(2) * p_batch.unsqueeze(1)  # (batch_size, n, n)\n",
    "            Q = torch.einsum(\"bijk,bjk->bi\", Y_batch, outer)  # (batch_size, n)\n",
    "            \n",
    "            # Compute linear term: L_i = sum_j W[i,j] p_j\n",
    "            L = torch.einsum(\"bij,bj->bi\", W_batch, p_batch)  # (batch_size, n)\n",
    "            \n",
    "            # Total drift function F = W p + p Y p\n",
    "            F = L + Q  # (batch_size, n)\n",
    "            \n",
    "            # Gradient descent step toward F(p) = 0\n",
    "            p_batch = p_batch + step_size * F\n",
    "            \n",
    "            # Project onto probability simplex: clamp and normalize\n",
    "            p_batch = torch.clamp(p_batch, min=eps)\n",
    "            p_batch = p_batch / p_batch.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Handle NaN/Inf (fallback to uniform)\n",
    "        mask = torch.isnan(p_batch).any(dim=1) | torch.isinf(p_batch).any(dim=1)\n",
    "        if mask.any():\n",
    "            p_batch[mask] = 1.0 / n\n",
    "        \n",
    "        return p_batch\n",
    "\n",
    "\n",
    "def direct_solve_steady_state_ext_mom(model, W_batch, Y_batch, n_iter=50, step_size=0.05, eps=1e-12, tol=1e-6, momentum=0.3):\n",
    "        \"\"\"\n",
    "        Solve nonlinear steady state using fixed-point iteration (gradient descent).\n",
    "        \n",
    "        Solves: W p + p Y p = 0 with sum(p) = 1 and p >= 0\n",
    "        where (p Y p)_i = sum_{j,k} Y[i,j,k] p_j p_k\n",
    "        \n",
    "        Uses gradient descent to minimize ||W p + p Y p||, projecting onto the probability simplex.\n",
    "        \n",
    "        Args:\n",
    "            W_batch: (batch_size, n_nodes, n_nodes) - linear rate matrix\n",
    "            Y_batch: (batch_size, n_nodes, n_nodes, n_nodes) - nonlinear rate tensor\n",
    "            n_iter: Number of fixed-point iterations (default: 50)\n",
    "            step_size: Step size for gradient descent (default: 0.1)\n",
    "            eps: Minimum value for clamping (default: 1e-12)\n",
    "            \n",
    "        Returns:\n",
    "            p_batch: (batch_size, n_nodes) - steady state distributions\n",
    "        \"\"\"\n",
    "        batch_size, n = W_batch.shape[0], model.n_nodes\n",
    "        device = W_batch.device\n",
    "        \n",
    "        # Initialize from uniform distribution\n",
    "        p_batch = torch.full((batch_size, n), 1.0/n, device=device)\n",
    "        \n",
    "        # Momentum buffer\n",
    "        velocity = torch.zeros_like(p_batch)\n",
    "        \n",
    "        for iter_idx in range(n_iter):\n",
    "            # Compute quadratic term: Q_i = sum_{j,k} Y[i,j,k] p_j p_k\n",
    "            outer = p_batch.unsqueeze(2) * p_batch.unsqueeze(1)  # (batch_size, n, n)\n",
    "            Q = torch.einsum(\"bijk,bjk->bi\", Y_batch, outer)  # (batch_size, n)\n",
    "            \n",
    "            # Compute linear term: L_i = sum_j W[i,j] p_j\n",
    "            # Using bmm is slightly faster than einsum for matrix-vector multiply\n",
    "            L = torch.bmm(W_batch, p_batch.unsqueeze(-1)).squeeze(-1)  # (batch_size, n)\n",
    "            \n",
    "            # Total drift function F = W p + p Y p\n",
    "            F = L + Q  # (batch_size, n)\n",
    "            \n",
    "            # Check for early convergence (optional)\n",
    "            max_drift = torch.abs(F).max().item()\n",
    "            if max_drift < tol:\n",
    "                break\n",
    "            \n",
    "            # Momentum update\n",
    "            velocity = momentum * velocity + step_size * F\n",
    "            p_batch = p_batch + velocity\n",
    "            \n",
    "            # Project onto probability simplex: clamp and normalize\n",
    "            p_batch = torch.clamp(p_batch, min=eps)\n",
    "            p_batch = p_batch / p_batch.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Handle NaN/Inf (fallback to uniform)\n",
    "        mask = torch.isnan(p_batch).any(dim=1) | torch.isinf(p_batch).any(dim=1)\n",
    "        if mask.any():\n",
    "            p_batch[mask] = 1.0 / n\n",
    "        \n",
    "        return p_batch\n",
    "\n",
    "def newton_steady_state(model, W_batch, Y_batch, n_iter=10, eps=1e-12, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Newton's method for solving W p + p Y p = 0.\n",
    "    \n",
    "    Much faster convergence (typically 5-10 iterations vs 50+)\n",
    "    \"\"\"\n",
    "    batch_size, n = W_batch.shape[0], model.n_nodes\n",
    "    device = W_batch.device\n",
    "    \n",
    "    # Initialize from uniform distribution\n",
    "    p_batch = torch.full((batch_size, n), 1.0/n, device=device)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        # Compute F(p) = W p + p Y p\n",
    "        outer = p_batch.unsqueeze(2) * p_batch.unsqueeze(1)\n",
    "        Q = torch.einsum(\"bijk,bjk->bi\", Y_batch, outer)\n",
    "        L = torch.bmm(W_batch, p_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        F = L + Q\n",
    "        \n",
    "        # Check convergence\n",
    "        if torch.abs(F).max() < tol:\n",
    "            break\n",
    "        \n",
    "        # Compute Jacobian: J[i,j] = dF_i/dp_j\n",
    "        # J = W + (Y[:,:,j,k] + Y[:,i,k,j]) * p_k (summed over k)\n",
    "        # For quadratic term: d/dp_j (sum_k,l Y[i,k,l] p_k p_l) = sum_l Y[i,j,l]*p_l + sum_k Y[i,k,j]*p_k\n",
    "        \n",
    "        # Simplified: J = W + 2 * (Y contracted with p on last index)\n",
    "        Y_p_contract = torch.einsum(\"bijk,bk->bij\", Y_batch, p_batch)  # (b, n, n)\n",
    "        Y_p_contract_T = torch.einsum(\"bijk,bj->bik\", Y_batch, p_batch)  # (b, n, n)\n",
    "        J = W_batch + Y_p_contract + Y_p_contract_T  # (batch, n, n)\n",
    "        \n",
    "        # Add constraint: last row enforces sum(p) = 1\n",
    "        J_constrained = J.clone()\n",
    "        J_constrained[:, -1, :] = 1.0\n",
    "        \n",
    "        # RHS\n",
    "        b = -F\n",
    "        b[:, -1] = 1.0 - p_batch.sum(dim=1)  # Enforce sum constraint\n",
    "        \n",
    "        # Solve J * delta_p = -F\n",
    "        delta_p = torch.linalg.solve(J_constrained, b)\n",
    "        \n",
    "        # Update with line search damping\n",
    "        alpha = 1.0\n",
    "        p_new = p_batch + alpha * delta_p\n",
    "        p_new = torch.clamp(p_new, min=eps)\n",
    "        p_new = p_new / p_new.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        p_batch = p_new\n",
    "    \n",
    "    return p_batch\n",
    "\n",
    "def newton_steady_state_optimized(model, W_batch, Y_batch, n_iter=10, eps=1e-12, tol=1e-8, reg=1e-6):\n",
    "    \"\"\"\n",
    "    Optimized Newton's method with several improvements.\n",
    "    \"\"\"\n",
    "    batch_size, n = W_batch.shape[0], model.n_nodes\n",
    "    device = W_batch.device\n",
    "    \n",
    "    # Initialize\n",
    "    p_batch = torch.full((batch_size, n), 1.0/n, device=device)\n",
    "    \n",
    "    # ===== KEY OPTIMIZATION 1: Pre-compute symmetric Y tensor =====\n",
    "    # Since Jacobian needs Y[i,j,k] + Y[i,k,j], compute once outside loop\n",
    "    Y_sym = Y_batch + Y_batch.transpose(1, 2)  # (batch, n, n, n)\n",
    "    \n",
    "    # ===== KEY OPTIMIZATION 2: Pre-allocate tensors =====\n",
    "    outer = torch.empty(batch_size, n, n, device=device)\n",
    "    J = torch.empty(batch_size, n, n, device=device)\n",
    "    b = torch.empty(batch_size, n, device=device)\n",
    "    \n",
    "    # Pre-compute identity for regularization\n",
    "    eye_reg = reg * torch.eye(n, device=device)\n",
    "    \n",
    "    for iter_idx in range(n_iter):\n",
    "        # ===== Compute F(p) = W p + p Y p =====\n",
    "        outer = p_batch.unsqueeze(2) * p_batch.unsqueeze(1)  # (batch, n, n)\n",
    "        Q = torch.einsum(\"bijk,bjk->bi\", Y_batch, outer)\n",
    "        L = torch.bmm(W_batch, p_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        F = L + Q\n",
    "        \n",
    "        # ===== Early exit =====\n",
    "        max_drift = torch.abs(F).max().item()\n",
    "        if max_drift < tol:\n",
    "            break\n",
    "        \n",
    "        # ===== KEY OPTIMIZATION: Single einsum for Jacobian =====\n",
    "        J = W_batch + torch.einsum(\"bijk,bk->bij\", Y_sym, p_batch) + eye_reg\n",
    "        \n",
    "        # ===== Constraint: Replace last row =====\n",
    "        J[:, -1, :] = 1.0\n",
    "        \n",
    "        # ===== RHS =====\n",
    "        b = -F.clone()\n",
    "        b[:, -1] = 1.0 - p_batch.sum(dim=1)\n",
    "        \n",
    "        # ===== Solve =====\n",
    "        try:\n",
    "            delta_p = torch.linalg.solve(J, b)\n",
    "        except RuntimeError:\n",
    "            delta_p = torch.linalg.lstsq(J, b.unsqueeze(-1)).solution.squeeze(-1)\n",
    "        \n",
    "        # ===== Update (in-place where possible) =====\n",
    "        p_batch = p_batch + delta_p\n",
    "        p_batch.clamp_(min=eps)\n",
    "        p_batch.div_(p_batch.sum(dim=1, keepdim=True))\n",
    "    \n",
    "    return p_batch\n",
    "\n",
    "# def lbfgs_steady_state_batch(model, W_batch, Y_batch, max_iter=20, tol=1e-8):\n",
    "#     \"\"\"\n",
    "#     Apply L-BFGS separately to each batch element.\n",
    "#     More flexible but slightly slower.\n",
    "#     \"\"\"\n",
    "#     batch_size, n = W_batch.shape[0], model.n_nodes\n",
    "#     device = W_batch.device\n",
    "    \n",
    "#     p_results = []\n",
    "    \n",
    "#     for b in range(batch_size):\n",
    "#         W_b = W_batch[b:b+1]  # Keep batch dimension\n",
    "#         Y_b = Y_batch[b:b+1]\n",
    "        \n",
    "#         logits = torch.zeros(1, n, device=device, requires_grad=True)\n",
    "        \n",
    "#         optimizer = torch.optim.LBFGS(\n",
    "#             [logits], \n",
    "#             max_iter=max_iter,\n",
    "#             tolerance_grad=tol,\n",
    "#             tolerance_change=tol,\n",
    "#             line_search_fn='strong_wolfe'\n",
    "#         )\n",
    "        \n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             p = torch.softmax(logits, dim=1)\n",
    "            \n",
    "#             outer = p.unsqueeze(1) * p.unsqueeze(2)\n",
    "#             Q = torch.einsum(\"bijk,bjk->bi\", Y_b, outer)\n",
    "#             L = torch.bmm(W_b, p.unsqueeze(-1)).squeeze(-1)\n",
    "#             F = L + Q\n",
    "            \n",
    "#             loss = (F ** 2).sum()\n",
    "#             loss.backward()\n",
    "#             return loss\n",
    "        \n",
    "#         optimizer.step(closure)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             p_results.append(torch.softmax(logits, dim=1))\n",
    "    \n",
    "#     return torch.cat(p_results, dim=0)\n",
    "        \n",
    "        \n",
    "        # Get a single example from the training dataset\n",
    "z_seq, labels, targets = train_loader.dataset[2000]\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "z_seq = z_seq.unsqueeze(0).to(device)  # Shape: (1, N+1, D)\n",
    "labels = labels.unsqueeze(0).to(device)  # Shape: (1, N)\n",
    "targets = targets.unsqueeze(0).to(device).long() - 1  # Shape: (1,)\n",
    "\n",
    "# print(\"Example input:\")\n",
    "# print(f\"z_seq shape: {z_seq.shape}\")\n",
    "# print(f\"labels shape: {labels.shape}\")\n",
    "# print(f\"z_seq:\\n{z_seq}\")\n",
    "# print(f\"labels: {labels}\")\n",
    "\n",
    "# Flatten z_seq as the model expects\n",
    "z_flat = z_seq.reshape(1, -1)  # Shape: (1, (N+1)*D)\n",
    "# print(f\"\\nFlattened z shape: {z_flat.shape}\")\n",
    "\n",
    "# Compute the rate matrix W and rate tensor Y\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    W_mat = model.compute_rate_matrix_W(z_flat)\n",
    "    Y_tensor = model.compute_rate_tensor_Y(z_flat)\n",
    "\n",
    "print(f\"\\nRate matrix W shape: {W_mat.shape}\")\n",
    "print(f\"Rate matrix W:\\n{W_mat[0]}\")  # Print the first (and only) matrix in the batch\n",
    "\n",
    "# Verify that columns sum to approximately zero (as they should for a rate matrix)\n",
    "col_sums = W_mat[0].sum(dim=0)\n",
    "# print(f\"\\nColumn sums of W (should be ~0): {col_sums}\")\n",
    "\n",
    "# print(f\"\\nRate tensor Y shape: {Y_tensor.shape}\")\n",
    "# print(f\"Rate tensor Y[0] (first batch):\\n{Y_tensor[0]}\")\n",
    "\n",
    "# Verify the constraint: Y[j,j,k] = -sum_{i≠j} Y[i,j,k]\n",
    "# Check for j=0, k=0 as an example\n",
    "j, k = 0, 0\n",
    "sum_off_diag = Y_tensor[0, :, j, k].sum() - Y_tensor[0, j, j, k]  # Sum excluding diagonal\n",
    "# print(f\"\\nConstraint check for Y[j={j},j={j},k={k}]:\")\n",
    "# print(f\"  Y[{j},{j},{k}] = {Y_tensor[0, j, j, k].item():.6f}\")\n",
    "# print(f\"  -Sum_{{i≠{j}}} Y[i,{j},{k}] = {-sum_off_diag.item():.6f}\")\n",
    "# print(f\"  Difference (should be ~0): {(Y_tensor[0, j, j, k] + sum_off_diag).item():.6e}\")\n",
    "\n",
    "# Compute steady state using nonlinear solver\n",
    "n_iter = 10\n",
    "step_size = 0.05\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        #p_steady = model.direct_solve_steady_state(W_mat, Y_tensor, n_iter=n_iter, step_size=step_size)\n",
    "        p_steady = model.newton_steady_state(W_mat, Y_tensor, n_iter=n_iter)\n",
    "        #p_steady =  newton_steady_state_optimized(model, W_mat, Y_tensor, n_iter=n_iter, reg = 0)\n",
    "        #p_steady = newton_steady_state(model,W_mat, Y_tensor, n_iter=n_iter)\n",
    "        #p_steady = newton_steady_state(model, W_mat, Y_tensor, n_iter=n_iter)\n",
    "        #p_steady = lbfgs_steady_state_batch(model, W_mat, Y_tensor, max_iter=n_iter, tol = )\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "print(f\"\\nSteady state distribution (π):\")\n",
    "print(f\"Shape: {p_steady.shape}\")\n",
    "print(f\"π = {p_steady[0]}\")\n",
    "print(f\"Sum of π (should be 1.0): {p_steady[0].sum().item()}\")\n",
    "\n",
    "# Verify the steady state: W p + p Y p should be approximately 0\n",
    "with torch.no_grad():\n",
    "    # Linear term: W @ p\n",
    "    linear_term = torch.einsum(\"bij,bj->bi\", W_mat, p_steady)\n",
    "    \n",
    "    # Quadratic term: sum_{j,k} Y[i,j,k] * p_j * p_k\n",
    "    outer = p_steady.unsqueeze(2) * p_steady.unsqueeze(1)  # (batch_size, n, n)\n",
    "    quadratic_term = torch.einsum(\"bijk,bjk->bi\", Y_tensor, outer)\n",
    "    \n",
    "    # Total should be ~0\n",
    "    total_drift = linear_term + quadratic_term\n",
    "    \n",
    "print(f\"\\nSteady state verification (W p + p Y p):\")\n",
    "print(f\"Linear term (W @ p): {linear_term[0]}\")\n",
    "print(f\"Quadratic term (p Y p): {quadratic_term[0]}\")\n",
    "print(f\"Total (should be ~0): {total_drift[0]}\")\n",
    "print(f\"Max absolute drift: {torch.abs(total_drift[0]).max().item():.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
